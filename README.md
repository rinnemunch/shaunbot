# ğŸ¤– Shaunbot

A local AI chatbot powered by LLaMA 3 (via Ollama) with a smooth PyQt interface.  
Built to run entirely offline with fast, threaded responses and a natural conversation flow.

---

## ğŸš€ Features

âœ… Chat with LLaMA 3 locally â€” no API key needed  
âœ… Conversation memory (context-aware replies)  
âœ… Smooth UI built with PyQt5  
âœ… Typing animation for realistic replies  
âœ… Clear Chat button to reset the convo  
âœ… Fully threaded â€” no lag or freezing  

---

## ğŸ–¼ï¸ Screenshot

![Shaunbot Demo](Screenshot.png)

---

## ğŸ› ï¸ How to Run

1. Clone the repo:
```bash
git clone https://github.com/rinnemunch/shaunbot.git
cd shaunbot
````
2. Create and activate a virtual environment: 
```bash
python -m venv venv
source venv/bin/activate        
# Windows: .\venv\Scripts\activate
```` 

3. Install dependencies: 
```bash
pip install -r requirements.txt
````  

4. Make sure Ollama is installed and running: 
```bash
ollama run llama3
```` 

5. Launch Shaunbot: 
```bash
python main.py
```` 

ğŸ“¦ Requirements 
- PyQt5

- requests

- Ollama (for running LLaMA 3 locally) 

ğŸ’¡ Notes 
- All replies are generated locally â€” you donâ€™t need an internet connection after the model is downloaded

- Uses QThread to avoid UI blocking

- Typing animation is customizable (via QTimer) 

ğŸ”“ License 
MIT â€” use it, remix it, break it, improve it.